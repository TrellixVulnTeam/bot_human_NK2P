{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit ('test': venv)",
   "display_name": "Python 3.6.9 64-bit ('test': venv)",
   "metadata": {
    "interpreter": {
     "hash": "fcf41e46691bb2ec99426d359e76e955c532cdf03bc21b806fcc710907b61136"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pre-procesamiento del dataset\n",
    "### Dylan Javier Primera  T00045753\n",
    "\n",
    "### Romario Marimon Romero T00049321\n",
    "\n",
    "### Camilo Dario Bautista T00044509"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "source": [
    "## lectura del archivo de texto \n",
    "\n",
    "Pasamos la ruta donde se encuentra, lo por linea y almacenamos en la lista all_truth la información"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./parcial1/truth.txt','r')\n",
    "data = []\n",
    "while(True):\n",
    "\tline = f.readline()\n",
    "\tif not line:\n",
    "\t\tbreak\n",
    "\tdata.append(line)\n",
    "\n",
    "f.close()\n",
    "\n",
    "all_truth = []\n",
    "\n",
    "\n",
    "n = len(data)\n",
    "for i in range(n):\n",
    "\tdata[i] = data[i].replace('\\n','')\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "\tnew_data = data[i].split(':::')\n",
    "\tall_truth.append(new_data)"
   ]
  },
  {
   "source": [
    "# Creación del nuevo dataset(pre-procesado)\n",
    "tenemos 3 funciones get_tweet() la cual leeara todos los tweets que un usuario en espesifico realizó, get_names() que retornará una lista con los nombres de todos los archivos xml's sabiendo que estos son los id's de los usuarios, y por último is_human(), la cual retornará cero si el id pasado por parametro pertence a un robot o uno si pertenece a un humano.\n",
    "\n",
    "\n",
    "Finalmente utilizamos las funciones antes vistas para obtener todos los tweets de cada usuario y agregar si este es un humano o un robot "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 4/3000 [00:00<01:19, 37.89it/s]\n",
      "Writing a csv file\n",
      "100%|██████████| 3000/3000 [00:31<00:00, 96.69it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_tweet(path):\n",
    "    doc = ET.parse(path)\n",
    "    root = doc.getroot()\n",
    "    tweets = [i.text for i in root.iter('document')]\n",
    "    return tweets\n",
    "\n",
    "def get_names(path):\n",
    "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    return onlyfiles\n",
    "\n",
    "\n",
    "def is_humam(user_id,all_truth):\n",
    "    n = len(all_truth)\n",
    "    for i in range(n):\n",
    "        if(user_id in all_truth[i]):\n",
    "            aux = all_truth[i]\n",
    "            if(aux[1]=='bot'):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "            break\n",
    "          \n",
    "\n",
    "path = './parcial1/xml'\n",
    "all_names =  get_names(path)\n",
    "all_names_copy = all_names.copy() # tiene .xml\n",
    "n = len(all_names)\n",
    "for i in range(n):\n",
    "    all_names[i] = all_names[i].replace('.xml','')\n",
    "\n",
    "\n",
    "n = len(all_names)\n",
    "print('\\nWriting a csv file')\n",
    "with open('data.csv', 'w') as file:\n",
    "    writer = csv.writer(file,lineterminator='\\n')\n",
    "    writer.writerow(['user_id','tweets','is_human'])\n",
    "    for i in tqdm(range(n)):\n",
    "        path2 = './parcial1/xml/'\n",
    "        actual_name = all_names_copy[i]\n",
    "        username = all_names[i]\n",
    "        human = is_humam(username,all_truth)\n",
    "        path2 += actual_name\n",
    "        tweets = get_tweet(path2)\n",
    "        writer.writerow([username,tweets,human])"
   ]
  }
 ]
}